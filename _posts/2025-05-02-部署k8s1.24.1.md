---
title: 部署k8s
date: 2025-05-02 22:34:00 +0800
categories: [k8s,部署k8s]
tags: [k8s,部署k8s]
toc: true
---

3台机器配置为2G2核
主机名   操作系统版本     ip             kubeadm/kubelet/kubectl version
master    7.9.2009  192.168.189.200                        1.23.9
node1     7.9.2009  192.168.189.201                        1.23.9
node2     7.9.2009  192.168.189.202                        1.23.9
#### 一、部署前工作
每台机器至少2核2G，不能有重复的主机名,MAC和product_uuid。
以下每台节点都有执行
##### 1. 禁用selinux关闭防火墙
##### 2. 设置主机名并添加hostname到hosts文件中
##### 3. 设置对时及dns及yum源
```
curl -o /etc/yum.repos.d/Centos-7.repo   https://mirrors.aliyun.com/repo/Centos-7.repo
curl -o /etc/yum.repos.d/epel-7.repo   https://mirrors.aliyun.com/repo/epel-7.repo
yum clean all && yum makecache
初次yum makecache时在生成centos源缓存时报错
base/7/x86_64/filelists_db     FAILED                                          
http://mirrors.aliyuncs.com/centos/7/os/x86_64/repodata/d6d94c7d406fe7ad4902a97104b39a0d8299451832a97f31d71653ba982c955b-filelists.sqlite.bz2:
 [Errno 14] curl#7 - "Failed connect to mirrors.aliyuncs.com:80; Connection refused"Trying other mirror.
查看Centos-7.repo文件为yum源为http，修改为https后再次执行makecahce正常
[root@master yum.repos.d]# sed -i.bak 's/http/https/g' Centos-7.repo
[root@master yum.repos.d]# yum clean all && yum makecache
yum install -y yum-utils device-mapper-persistent-data lvm2 wegt net-tools vim sshpass
```

##### 4. 禁用swap
临时关闭；关闭swap主要是为了性能考虑
swapoff -a
可以通过这个命令查看swap是否关闭了
free
永久关闭
sed -ri 's/.*swap.*/#&/' /etc/fstab
##### 5. 模块加载
查看br_netfilter 模块是否已加载
lsmod | grep br_netfilter
如果没有加载，执行命令加载
```
sudo modprobe br_netfilter
sudo modprobe overlay
cat <<EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF
```
##### 6. 修改内核参数
为了让 Linux 节点的 iptables 能够正确查看桥接流量，需要修改以下内核参数
```
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
vm.swappiness                       = 0
EOF
sudo sysctl -p
```


#### 二、安装容器运行时
以下每台节点都有执行
1.24版本及以后需要安装符合容器运行时接口（CRI）的运行时，例举了如下几种

1. containerd
2. CRI-O
3. Docker Engine + cri-dockerd

##### 1.使用containerd作为容器运行时,直接yum安装
添加docker-ce源
`yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo`
安装docker-ce版本并设置开机自启
```
yum list containerd --showduplicates #查看版本
yum install -y containerd         #指定版本安装
systemctl enable --now containerd       #设置开机自启
```

##### 2.配置cgroup驱动
在 Linux 上，控制组（CGroup）用于限制分配给进程的资源。
可用的cgroup驱动有2个
1. cgroupfs
2. systemd

一般都使用systemd作为kubelet的cgroup驱动，容器运行时的cgroup驱动应和kubelet保持一致
###### 1）更改容器运行时的cgroup驱动为systemd
yum安装的containerd默认没有配置文件，执行以下命令导出配置文件
containerd config default > /etc/containerd/config.toml
执行命令直接修改cgroup驱动
```
sed -i 's#SystemdCgroup = false#SystemdCgroup = true#g' /etc/containerd/config.toml
```
###### 2）更改沙箱镜像为阿里云的镜像
更改registry为阿里云的registry
```
sed -i "s#k8s.gcr.io/pause#registry.aliyuncs.com/google_containers/pause#g" /etc/containerd/config.toml
```
###### 3）修改socket路径
在 Linux 上，containerd 的默认 CRI 套接字是 /run/containerd/containerd.sock
kubeadm默认找的套接字路径为/var/run/containerd/containerd.sock，可以在kubeadm init时使用--cri-socket指定，也可以更改`containerd`的`默认CRI 套接字`
```
sed '/sock/s#/run#/var/run#g' config.toml
```
###### 4）删除disabled_plugins中的cri
确认disabled_plugins = []没有cri,如果有cri需要删除
###### 5）设置registry为阿里云的registry
在registry.mirrors下添加2行
```
      [plugins."io.containerd.grpc.v1.cri".registry.mirrors]
        [plugins."io.containerd.grpc.v1.cri".registry.mirrors."docker.io"]
        endpoint = ["http://registry.aliyuncs.com/google_containers","https://registry-1.docker.io"]
```

###### 6）查看更改后的 /etc/containerd/config.toml
```
disabled_plugins = []
[grpc]
  address = "/var/run/containerd/containerd.sock"
#  address = "/run/containerd/containerd.sock"
  [plugins."io.containerd.grpc.v1.cri"]
    sandbox_image = "registry.aliyuncs.com/google_containers/pause:3.6"
...
        [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
...
          [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
            SystemdCgroup = true
...
    [plugins."io.containerd.grpc.v1.cri".registry]
      [plugins."io.containerd.grpc.v1.cri".registry.mirrors]
        [plugins."io.containerd.grpc.v1.cri".registry.mirrors."docker.io"]
        endpoint = ["http://registry.aliyuncs.com/google_containers","https://registry-1.docker.io"]

```

###### 7）如果runtime是docker
设置镜像加速，修改cgroup为systemd
[root@master ~]# cat >>/etc/docker/daemon.json <<EOF
{
  "registry-mirrors": ["https://v16stybc.mirror.aliyuncs.com"],
  "exec-opts": ["native.cgroupdriver=systemd"]
}
EOF
###### 8）加载重启对应的服务
systemctl daemon-reload 
systemctl restart containerd
systemctl restart docker
###### 9）检查当前runtime配置，是否修改成功
containerd config dump
```
$ docker info | grep -in1  reg
47- Debug Mode: false
48: Registry: https://index.docker.io/v1/
49- Labels:
50- Experimental: false
51: Insecure Registries:
52-  127.0.0.0/8
53: Registry Mirrors:
54-  https://u96790vf.mirror.aliyuncs.com/
$ docker info | grep -i dri
 Storage Driver: overlay2
 Logging Driver: json-file
 Cgroup Driver: systemd
 ```

#### 三、安装 kubeadm、kubelet 和 kubectl
以下每台节点都有执行

    kubeadm：用来初始化集群的指令。

    kubelet：在集群中的每个节点上用来启动 Pod 和容器等。

    kubectl：用来与集群通信的命令行工具。node节点不执行kubectl命令不需要安装

##### 1.配置好k8s源，直接yum安装，各组件版本必须一致
```
cat > /etc/yum.repos.d/kubernetes.repo << EOF
[k8s]
name=k8s
enabled=1
gpgcheck=0
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
EOF
yum list kubelet --showduplicates
sudo yum install -y kubelet-1.24.1  kubeadm-1.24.1  kubectl-1.24.1 --disableexcludes=kubernetes
```
##### 2.kubelet设置开机自启
设置开机自启，此时kubelet会一直重启，等kubeadm init或kube join后就会正常
```
sudo systemctl enable --now kubelet
```
##### 3.配置shell自动补全功能
yum安装bash-completion包，启动kubectl自动补全功能
yum install -y bash-completion
当前用户
```
echo 'source <(kubectl completion bash)' >>~/.bashrc
```
系统全局
```
kubectl completion bash | sudo tee /etc/bash_completion.d/kubectl > /dev/null
```
添加别名
```
echo 'alias ka=kubectl' >>~/.bashrc
```
source 写入的文件
##### 4.更改kubelet的cgroup驱动为systemd
###### 1)部署时设置
kubeadm 支持在执行 kubeadm init 时，传递一个 KubeletConfiguration 结构体。 KubeletConfiguration 包含 cgroupDriver 字段，可用于控制 kubelet 的 cgroup 驱动。没有设置cgroupdriver字段，默认值为systemd,也可以显示设置
显示设置
```
cat kubeadm-config.yaml
kind: ClusterConfiguration
apiVersion: kubeadm.k8s.io/v1beta3
kubernetesVersion: v1.21.0
---
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
cgroupDriver: systemd
```
kubeadm初始化完成后，可以在/var/lib/kubelet/config.yaml文件查看`cgroupDriver`是否为`systemd`

###### 2)部署后设置
* a. 修改 kubelet 的 ConfigMap
    运行 kubectl edit cm kubelet-config -n kube-system。
    修改现有 cgroupDriver 的值，或者新增如下式样的字段：
    cgroupDriver: systemd
    该字段必须出现在 ConfigMap 的 kubelet: 小节下。
* b. 更新所有节点的 cgroup 驱动
    对于集群中的每一个节点：
    执行命令 kubectl drain <node-name> --ignore-daemonsets --force，以 腾空节点  
    执行命令 systemctl stop kubelet，以停止 kubelet  
    停止容器运行时  
    修改容器运行时 cgroup 驱动为 systemd  
    在文件 /var/lib/kubelet/config.yaml 中添加设置 cgroupDriver: systemd  
    启动容器运行时  
    执行命令 systemctl start kubelet，以启动 kubelet  
    执行命令 kubectl uncordon <node-name>，以 取消节点隔离  
在节点上依次执行上述步骤，确保工作负载有充足的时间被调度到其他节点。
#### 四、初始化集群
选一台master节点执行
##### 1.下载容器镜像
查看需要哪些镜像kubeadm config images list
```
[root@master ~]# kubeadm config images list
I0110 20:08:42.256070    6820 version.go:255] remote version is much newer: v1.29.0; falling back to: stable-1.24
k8s.gcr.io/kube-apiserver:v1.24.1
k8s.gcr.io/kube-controller-manager:v1.24.1
k8s.gcr.io/kube-scheduler:v1.24.1
k8s.gcr.io/kube-proxy:v1.24.1
k8s.gcr.io/pause:3.7
k8s.gcr.io/etcd:3.5.3-0
k8s.gcr.io/coredns/coredns:v1.8.6
```
查看国内镜像
```
[root@master1 ~]#kubeadm config images list --image-repository registry.aliyuncs.com/google_containers
registry.aliyuncs.com/google_containers/kube-apiserver:v1.24.1
registry.aliyuncs.com/google_containers/kube-controller-manager:v1.24.1
registry.aliyuncs.com/google_containers/kube-scheduler:v1.24.1
registry.aliyuncs.com/google_containers/kube-proxy:v1.24.1
registry.aliyuncs.com/google_containers/pause:3.7
registry.aliyuncs.com/google_containers/etcd:3.5.3-0
registry.aliyuncs.com/google_containers/coredns:v1.8.6
```
从国内镜像站拉取镜像
```
[root@master1 ~]#kubeadm config images pull --kubernetes-version=v1.24.1 --image-repository=registry.aliyuncs.com/google_containers
```
提前下载好容器镜像，init时就不会再拉取，init时指定国内仓库
```
docker pull registry.aliyuncs.com/google_containers/kube-apiserver:v1.24.1
docker pull registry.aliyuncs.com/google_containers/kube-controller-manager:v1.24.1
docker pull registry.aliyuncs.com/google_containers/kube-scheduler:v1.24.1
docker pull registry.aliyuncs.com/google_containers/kube-proxy:v1.24.1
docker pull registry.aliyuncs.com/google_containers/pause:3.7
docker pull registry.aliyuncs.com/google_containers/etcd:3.5.3-0
docker pull registry.aliyuncs.com/google_containers/coredns:v1.8.6
```
如果init时不指定国内仓库，可以下载镜像后修改tag
```
[root@master ~]# cat images.sh 
#!/bin/bash
url=registry.cn-hangzhou.aliyuncs.com/google_containers
version=v1.17.12
images=(`kubeadm config images list --kubernetes-version=$version|awk -F '/' '{print $2}'`)
for imagename in ${images[@]} ; do
  docker pull $url/$imagename
  docker tag $url/$imagename k8s.gcr.io/$imagename
  docker rmi -f $url/$imagename     
done
[root@master ~]# chmod +x images.sh
[root@master ~]# ./images.sh 
```
##### 2.控制平面(master)集群初始化
控制平面节点是运行控制平面组件的机器， 包括 etcd（集群数据库） 和 API 服务器 （命令行工具 kubectl 与之通信）。

>（推荐）如果计划将单个控制平面 kubeadm 集群升级成高可用， 你应该指定 --control-plane-endpoint 为所有控制平面节点设置共享端点。 端点可以是负载均衡器的 DNS 名称或 IP 地址。kubeadm 不支持将没有 --control-plane-endpoint 参数的单个控制平面集群转换为高可用性集群。
选择一个 Pod 网络插件，并验证是否需要为 kubeadm init 传递参数。 根据你选择的第三方网络插件，你可能需要设置 --pod-network-cidr 的值。 请参阅安装 Pod 网络附加组件。
（可选）kubeadm 试图通过使用已知的端点列表来检测容器运行时。 使用不同的容器运行时或在预配置的节点上安装了多个容器运行时，请为 kubeadm init 指定 --cri-socket 参数。 请参阅安装运行时。
（可选）除非另有说明，否则 kubeadm 使用与默认网关关联的网络接口来设置此控制平面节点 API server 的广播地址。 要使用其他网络接口，请为 kubeadm init 设置 --apiserver-advertise-address=<ip-address> 参数。 要部署使用 IPv6 地址的 Kubernetes 集群， 必须指定一个 IPv6 地址，例如 --apiserver-advertise-address=2001:db8::101。

--apiserver-advertise-address:可用于为控制平面节点的 API server 设置广播地址， 
--control-plane-endpoint:可用于为所有控制平面节点设置共享端点。可以修改 cluster-endpoint 以指向高可用性方案中的负载均衡器的地址。允许 IP 地址和可以映射到 IP 地址的 DNS 名称。

###### 1） 添加/etc/hosts解析
在/etc/hosts文件中添加如下行
192.168.189.200 cluster-endpoint
###### 2） 设置参数开始初始化
1. 指定yaml文件
打印初始化的默认值yaml文件到kubeadm-config.yaml文件下
kubeadm config print init-defaults > kubeadm-config.yaml
修改各参数，再指定yaml文件进行初始化
kubeadm init --config kubeadm-config.yaml
2. 命令行初始化
kubeadm init \
  --apiserver-advertise-address=192.168.189.100  \
  --image-repository=registry.aliyuncs.com/google_containers \
  --control-plane-endpoint=cluster-endpoint:16443 \
  --kubernetes-version=v1.24.1 \
  --service-cidr=10.1.0.0/16 \
  --pod-network-cidr=10.244.0.0/16 \
  --cri-socket=/run/containerd/containerd.sock  \
  --token-ttl=0 \
  --upload-certs
3. 如果需要创建高可用集群，建议初始化时加上`--upload-certs`参数，后面加入control-plane节点时就不需要手动拷贝ca证书了
当使用`--upload-certs`调用 kubeadm init 时，主控制平面的证书被加密并上传到 kubeadm-certs Secret 中，将在所有控制平面实例之间的共享证书上传到集群。

kubeadm config print init-defaults > init.yml  #生成默认配置文件
修改配置文件
```shell
[root@master1 ~]# cat init.yml
apiVersion: kubeadm.k8s.io/v1beta3
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 1.2.3.4   #修改为本机ip
  bindPort: 6443
nodeRegistration:
  criSocket: unix:///var/run/containerd/containerd.sock #修改crisocket为实际socket
  imagePullPolicy: IfNotPresent
  name: node              #修改为当前主机名
  taints: null
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta3
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: "192.168.10.100:6444"   # VIP：PORT
controllerManager: {}
dns: {}
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers  # 使用国内镜像仓库
kind: ClusterConfiguration
kubernetesVersion: 1.24.0      #版本号
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0/12
  podSubnet: 10.244.0.0/16    # pod子网，和Flannel中要一致
scheduler: {}
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: ipvs
```

初始化集群
kubeadm init --config init.yml --upload-certs

kubeadm 对集群所有的节点，使用相同的 KubeletConfiguration。 KubeletConfiguration 存放于 kube-system 命名空间下的某个 ConfigMap 对象中。
执行 init、join 和 upgrade 等子命令会促使 kubeadm 将 KubeletConfiguration 写入到文件 /var/lib/kubelet/config.yaml 中， 继而把它传递给本地节点的 kubelet。
**KubeletConfiguration参数说明**

    --kubernetes-version：#kubernetes程序组件的版本号，它必须要与安装的kubelet程序包的版本号相同
    --control-plane-endpoint：#多主节点必选项,用于指定控制平面的固定访问地址，可是IP地址或DNS名
    称，会被用于集群管理员及集群组件的kubeconfig配置文件的API Server的访问地址,如果是单主节点的控
    制平面部署时不使用该选项,注意:kubeadm 不支持将没有 --control-plane-endpoint 参数的单个控制
    平面集群转换为高可用性集群。
    --pod-network-cidr：#Pod网络的地址范围，其值为CIDR格式的网络地址，通常情况下Flannel网络插
    件的默认为10.244.0.0/16，Calico网络插件的默认值为192.168.0.0/16
    --service-cidr：#Service的网络地址范围，其值为CIDR格式的网络地址，默认为10.96.0.0/12；通
    常，仅Flannel一类的网络插件需要手动指定该地址
    --service-dns-domain string #指定k8s集群域名，默认为cluster.local，会自动通过相应的DNS
    服务实现解析
    --apiserver-advertise-address：#API 服务器所公布的其正在监听的 IP 地址。如果未设置，则使
    用默认网关的IP。apiserver通告给其他组件的IP地址，一般应该为Master节点的用于集群内部通信的IP地
    址，0.0.0.0表示此节点上所有可用地址,非必选项
    --image-repository string #设置镜像仓库地址，默认为 k8s.gcr.io,此地址国内可能无法访问,可
    以指向国内的镜像地址
    --token-ttl #共享令牌（token）的过期时长，默认为24小时，0表示永不过期；为防止不安全存储等原因
    导致的令牌泄露危及集群安全，建议为其设定过期时长。未设定该选项时，在token过期后，若期望再向集群
    中加入其它节点，可以使用如下命令重新创建token，并生成节点加入命令。kubeadm token create --
    print-join-command
    --ignore-preflight-errors=Swap” #若各节点未禁用Swap设备，还需附加选项“从而让kubeadm忽略
    该错误
    --upload-certs #将控制平面证书上传到 kubeadm-certs Secret
    --cri-socket #v1.24版之后指定连接cri的socket文件路径,注意;不同的CRI连接文件不同
    #如果是cRI是containerd，则使用--cri-socket unix:///run/containerd/containerd.sock
    #如果是cRI是docker，则使用--cri-socket unix:///var/run/cri-dockerd.sock
    #如果是CRI是CRI-o，则使用--cri-socket unix:///var/run/crio/crio.sock
    #注意:CRI-o与containerd的容器管理机制不一样，所以镜像文件不能通用。

###### 3） 初始化完成
```shell
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join cluster-endpoint:6443 --token b8ufi2.1qs3rw48dehh5yqx \
	--discovery-token-ca-cert-hash sha256:05fff7ddf0b5a6b9d4a7fd40de6d926019190773e8a067462625314d3548c987 \
	--control-plane \
  --certificate-key ae34c01f75e9971253a39543a289cdc651f23222c0e074a6b7ecb2dba667c059  #如果init时有--upload-certs参数会有这一行

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join cluster-endpoint:6443 --token b8ufi2.1qs3rw48dehh5yqx \
	--discovery-token-ca-cert-hash sha256:05fff7ddf0b5a6b9d4a7fd40de6d926019190773e8a067462625314d3548c987 
```

非root用户执行kubectl
```shell
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```
root 用户，则可以运行：
```shell
export KUBECONFIG=/etc/kubernetes/admin.conf
echo "export KUBECONFIG=/etc/kubernetes/admin.conf" >> ~/.bash_profile
source  ~/.bash_profile
```
非master节点执行kubectl命令，在node节点上执行
```shell
mkdir -p $HOME/.kube
sudo scp -rp master:/etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
如果admin.conf文件里的server写的是域名，需要在/etc/hosts文件中加入主机名解析
    server: https://cluster-endpoint:6443
```

#### 五、其他节点加入集群
##### 1.worker节点加入集群
###### 1）提前拉取镜像
```
docker pull registry.aliyuncs.com/google_containers/kube-proxy:v1.24.1
docker pull registry.aliyuncs.com/google_containers/pause:3.7
docker pull registry.aliyuncs.com/google_containers/coredns:v1.8.6
```
###### 2）执行master节点初始化以后输出的join命令
如果token过期，执行命令重新生成`kubeadm token create --print-join-command`加入集群的命令
```
[root@node1 ~]# kubeadm join cluster-endpoint:6443 --token b8ufi2.1qs3rw48dehh5yqx --discovery-token-ca-cert-hash sha256:05fff7ddf0b5a6b9d4a7fd40de6d926019190773e8a067462625314d3548c987 
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
```
##### 2.control-plane节点加入集群
###### 1）提前拉取镜像
```
docker pull registry.aliyuncs.com/google_containers/kube-apiserver:v1.24.1
docker pull registry.aliyuncs.com/google_containers/kube-controller-manager:v1.24.1
docker pull registry.aliyuncs.com/google_containers/kube-scheduler:v1.24.1
docker pull registry.aliyuncs.com/google_containers/kube-proxy:v1.24.1
docker pull registry.aliyuncs.com/google_containers/pause:3.7
docker pull registry.aliyuncs.com/google_containers/etcd:3.5.3-0
docker pull registry.aliyuncs.com/google_containers/coredns:v1.8.6
```
###### 2）直接加入
如果master初始化时加上了`--upload-certs`参数,可以直接添加--certificate-key \[certificate key\]加入control-plan节点到集群，不需要拷贝相关ca证书；
如果没有加上`--upload-certs`参数可以执行3，4步手动拷贝CA证书，也可以重新上传证书并生成新的解密密钥，然后在直接使用--certificate-key加入control-plan节点到集群。
重新上传证书并生成新的解密密钥，在已加入集群节点的control-plan节点上使用以下命令:
`sudo kubeadm init phase upload-certs --upload-certs`
**生成新的解密密钥**
```
[root@master ~]# kubeadm init phase upload-certs --upload-certs
I0219 15:01:27.684359   28058 version.go:255] remote version is much newer: v1.29.2; falling back to: stable-1.24
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
6bd4a0ea9a20e20ca96f85418d7fdf1b0f7115ebac721e2db06511a9a24ccc52
```
**加入集群**
--certificate-key为上面生成的解密密钥
```
[root@node1 ~]# kubeadm join cluster-endpoint:16443 --token 1uttug.r0hrf85kd2g1j112 --discovery-token-ca-cert-hash sha256:05fff7ddf0b5a6b9d4a7fd40de6d926019190773e8a067462625314d3548c987 --control-plane --certificate-key 6bd4a0ea9a20e20ca96f85418d7fdf1b0f7115ebac721e2db06511a9a24ccc52
```
###### 3）拷贝master节点上的相关证书到当前节点，再加入(手动证书发放)
当前节点创建目录，从master节点拷贝相关证书到当前节点
```
[root@node2 ~]# mkdir -p  /etc/kubernetes/pki/etcd
[root@master ~]# scp -rp /etc/kubernetes/pki/ca.* node2:/etc/kubernetes/pki
[root@master ~]# scp -rp /etc/kubernetes/pki/sa.* node2:/etc/kubernetes/pki
[root@master ~]# scp -rp /etc/kubernetes/pki/front-proxy-ca.* node2:/etc/kubernetes/pki
[root@master ~]# scp -rp /etc/kubernetes/pki/etcd/ca.* node2:/etc/kubernetes/pki/etcd
```
###### 4）执行master节点初始化以后输出的join命令，注意需要加上--control-plane参数
```
[root@node2 ~]# kubeadm join cluster-endpoint:16443 --token b8ufi2.1qs3rw48dehh5yqx --discovery-token-ca-cert-hash sha256:05fff7ddf0b5a6b9d4a7fd40de6d926019190773e8a067462625314d3548c987 --control-plane
[root@node2 ~]# mkdir -p $HOME/.kube
[root@node2 ~]# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
[root@node2 ~]# sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

    此时查看各node状态还是NotReady，需要安装完cni插件才会Ready
    [root@node2 kubernetes]# kubectl get nodes
    NAME     STATUS     ROLES           AGE   VERSION
    master   NotReady   control-plane   17h   v1.24.1
    node1    NotReady   <none>          15h   v1.24.1
    node2    NotReady   control-plane   28s   v1.24.1

#### 六、安装网络插件(cni)
安装完网络插件并且网络插件正常工作后，节点才会ready， coredns的pod才会是running状态
```
curl  https://docs.projectcalico.org/archive/v3.21/manifests/calico.yaml 
```
更改calico.yaml 配置，下面2行默认被注释，删除注释并删除一个空格，注意上下文的缩进是否正确！！！！！
            - name: CALICO_IPV4POOL_CIDR
              value: "10.244.0.0/16"           #此次网段为kubeadm init时指定的pod网段
如果拉取镜像比较慢，可以拉取阿里云镜像再改Tag
再查看node状态
```shell
[root@node2 kubernetes]# kubectl get nodes
NAME     STATUS     ROLES           AGE   VERSION
master   Ready   control-plane   17h   v1.24.1
node1    Ready   <none>          15h   v1.24.1
node2    Ready   control-plane   28s   v1.24.1
```
#### 七、删除master的污点
测试环境节点较少删除master的污点，让master可以参与调度
kubectl taint node  master1 node-role.kubernetes.io/control-plane- node-role.kubernetes.io/master-

#### 八、将kube-proxy的mode替换为ipvs
##### 1.开启kube-proxy开启ipvs的前置条件
安装了ipset软件包
[root@master ~]# yum install ipset -y

安装管理工具ipvsadm
```shell
[root@master ~]# yum install ipvsadm -y

cat >/etc/modules-load.d/ipvs.conf <<EOF
ip_vs
ip_vs_rr
ip_vs_wrr
ip_vs_sh
nf_conntrack
ip_tables
ip_set
xt_set
ipt_set
ipt_rpfilter
ipt_REJECT
ipip
EOF
```
加载模块
```shell
[root@master ~]# chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack_ipv4
```
##### 2.修改kube-proxy的mode

```shell
[root@master ~]# kubectl edit cm  kube-proxy -n kube-system
    mode: "ipvs"
```
默认为空，使用replace修改
```shell
[root@master ~]# kubectl get cm kube-proxy -n kube-system -oyaml | sed '/mode/s/""/"ipvs"/' | kubectl replace -f -
```
获取pod_name
```shell
[root@master ~]# kubectl get pods -A
kube-system    kube-proxy-rs6gc                 1/1     Running   1 (51m ago)   38h
kube-system    kube-proxy-sb8gj                 1/1     Running   2 (37h ago)   9d
kube-system    kube-proxy-wmqgg                 1/1     Running   2 (52m ago)   46h
```
删除旧的pod，因为是daemonset，删除了会重新在每个节点自动生成
```shell
[root@master ~]# kubectl delete pods kube-proxy-rs6gc -n kube-system
pod "kube-proxy-rs6gc" deleted
[root@master ~]# kubectl delete pods kube-proxy-sb8gj -n kube-system
pod "kube-proxy-sb8gj" deleted
[root@master ~]# kubectl delete pods kube-proxy-wmqgg -n kube-system
pod "kube-proxy-wmqgg" deleted
```
可以使用以下命令一次性删除
```shell
kubectl -n kube-system get pods | awk '/proxy/{print $1}' | while read line;do kubectl -n kube-system delete pods $line;done
```
在查看pod，能看到又重新生成了
```shell
[root@master ~]# kubectl get pods -A -w 
kube-system    kube-proxy-9kmvb                 1/1     Running   0             34s
kube-system    kube-proxy-9nc6b                 1/1     Running   0             23s
kube-system    kube-proxy-b6rs5                 1/1     Running   0             13s
```
此时执行`ipvsadm -l`也能看到已经生成的配置
[root@master ~]# ipvsadm -l
查看kube-proxy的mode
[root@master ~]# curl 127.0.0.1:10249/proxyMode
ipvs
#### 九、清理节点
如果节点有以前的k8s配置，需要清理以后再加入现在的k8s集群
##### 排空节点，在master节点上执行，清空以后会给该节点打上SchedulingDisabled标签
kubectl drain <node name> --delete-emptydir-data --force --ignore-daemonsets
##### 重置kubeadm安装的状态，在清空节点上执行
kubeadm reset
##### 重置iptables规则，在清空节点上执行
iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X
##### 重置IPVS表，在清空节点上执行
ipvsadm -C
##### 删除cni，在清空节点上执行
rm -rf /etc/cni/net.d/
##### 删除kube相关配置文件
rm -rf .kube/
##### 删除节点，在master节点上执行
kubectl delete node <节点名称>
##### 重新加入集群
kubectl uncordon <node-name>