---
title: k8s部署常见报错
date: 2025-05-03 22:37:00 +0800
categories: [k8s,k8s部署常见报错]
tags: [k8s,k8s部署常见报错]
[toc]
---

##### 一、kubeadm init时报错(swap is enable)
```
[WARNING Swap]: swap is enabled; production deployments should disable swap unless testing the NodeSwap feature gate of the kubelet
```
解决方案，执行如下命令，临时关闭swap
swapoff -a
永久关闭swap
sed -i '/swap/s/^/#/g' /etc/fstab

##### 二、kubeadm init时报错(container runtime is not running)
```
error execution phase preflight: [preflight] Some fatal errors occurred:
[ERROR CRI]: container runtime is not running: output: time="2023-07-21T09:20:07Z" level=fatal msg="validate service connection: CRI v1 runtime API is not implemented for endpoint \"unix:///var/run/containerd/containerd.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService"
, error: exit status 1
```
查看当前containerd的配置containerd config dump,确认是以下哪个问题

1. 导出默认配置，config.toml这个文件默认是不存在的
    containerd config default > /etc/containerd/config.toml
    查看配置文件cat /etc/containerd/config.toml,如果禁用了cri插件，删除cri
    disabled_plugins = ["cri"]
    将cri删除重启containerd服务
    systemctl daemon-reload
    systemctl restart containerd
2. 查看配置文件cat /etc/containerd/config.toml
```
    [grpc]
    address = "/run/containerd/containerd.sock"
    修改如下
    [grpc]
    address = "/var/run/containerd/containerd.sock"
    # address = "/run/containerd/containerd.sock"
    重启containerd服务
    systemctl daemon-reload
    systemctl restart containerd
```

##### 三、kubeadm init时报错(node \"master\" not found)
```
[kubelet.go:2419] "Error getting node" err="node \"master\" not found"
```
1. --apiserver-advertise-address=检查ip地址是否写正确

2. 查看主机名和/etc/hosts文件，主机名解析是否正确
kubeadm init \
  --apiserver-advertise-address=192.168.189.200  \
  --image-repository registry.aliyuncs.com/google_containers \
  --control-plane-endpoint=cluster-endpoint \
  --kubernetes-version v1.24.1 \
  --service-cidr=10.1.0.0/16 \
  --pod-network-cidr=10.244.0.0/16 \
  --v=5
此次使用的--control-plane-endpoint为设置的dns，设置为dns是为了后面部署为高可用集群，我没有在hosts文件中设置dns解析导致一直报这个错，添加如下解析，重新init就正常了
[root@master ~]# cat /etc/hosts
192.168.189.200 cluster-endpoint


3. 需要配置cri-docker.service文件，ExecStart=/usr/bin/cri-dockerd项后面指定你的指定你的pause版本，没遇到过
例如：–pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.7
cat /usr/lib/systemd/system/cri-docker.service 
ExecStart=/usr/bin/cri-dockerd --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.7 --container-runtime-endpoint
重启服务
systemctl daemon-reload && systemctl restart cri-docker.service

##### 四、kubeadm join时报错(/usr/bin/kubeadm: No such file or directory)
```
[root@k8snode2 ~]# kubeadm join cluster-endpoint:6443 --token b8ufi2.1qs3rw48dehh5yqx \
> --discovery-token-ca-cert-hash sha256:05fff7ddf0b5a6b9d4a7fd40de6d926019190773e8a067462625314d3548c987 \
> --control-plane 
-bash: /usr/bin/kubeadm: No such file or directory
```
yum安装kubeadm-1.24.1，会自动安装kubelet和kubectl，自动安装的kubectl和kubelet为新版本，需要指定具体版本安装

##### 五、kubeadm join时报错(hostname "k8snode2" could not be reached)
```
[root@k8snode2 ~]# kubeadm join cluster-endpoint:6443 --token b8ufi2.1qs3rw48dehh5yqx \
> --discovery-token-ca-cert-hash sha256:05fff7ddf0b5a6b9d4a7fd40de6d926019190773e8a067462625314d3548c987 \
> --control-plane 
[preflight] Running pre-flight checks
	[WARNING Hostname]: hostname "k8snode2" could not be reached
	[WARNING Hostname]: hostname "k8snode2": lookup k8snode2 on 192.168.189.2:53: no such host
error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR FileAvailable--etc-kubernetes-kubelet.conf]: /etc/kubernetes/kubelet.conf already exists
	[ERROR FileAvailable--etc-kubernetes-bootstrap-kubelet.conf]: /etc/kubernetes/bootstrap-kubelet.conf already exists
	[ERROR Port-10250]: Port 10250 is in use
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher
```
主机名不对，hosts写的是node2
[root@k8snode2 ~]# hostnamectl set-hostname node2
[root@node2 ~]# su
##### 六、kubeadm join时报错(kubelet.conf already exists)
```
[root@node2 ~]# kubeadm join cluster-endpoint:6443 --token b8ufi2.1qs3rw48dehh5yqx \
> --discovery-token-ca-cert-hash sha256:05fff7ddf0b5a6b9d4a7fd40de6d926019190773e8a067462625314d3548c987 \
> --control-plane 
[preflight] Running pre-flight checks
error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR FileAvailable--etc-kubernetes-kubelet.conf]: /etc/kubernetes/kubelet.conf already exists
	[ERROR FileAvailable--etc-kubernetes-bootstrap-kubelet.conf]: /etc/kubernetes/bootstrap-kubelet.conf already exists
	[ERROR Port-10250]: Port 10250 is in use
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher
```
配置文件已存在，删除相关配置
[root@node2 ~]# rm -rf /etc/kubernetes/
##### 七、kubeadm join时报错([ERROR Port-10250]: Port 10250 is in use)
```
[root@node2 ~]# kubeadm join cluster-endpoint:6443 --token b8ufi2.1qs3rw48dehh5yqx --discovery-token-ca-cert-hash sha256:05fff7ddf0b5a6b9d4a7fd40de6d926019190773e8a067462625314d3548c987 --control-plane 
[preflight] Running pre-flight checks
error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR Port-10250]: Port 10250 is in use
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher
```
10250端口已被占用，查看kubelet是running状态
后来发现node2节点被其他集群使用了，里面有配置。。。。
[root@node2 ~]# kubeadm reset
[root@master ~]# kubectl delete node node2
node "node2" deleted

##### 八、kubeadm join时报错(kubelet service is not enabled，[ERROR CRI]: container runtime is not running)
```
[root@node2 ~]#   kubeadm join cluster-endpoint:6443 --token b8ufi2.1qs3rw48dehh5yqx \
> --discovery-token-ca-cert-hash sha256:05fff7ddf0b5a6b9d4a7fd40de6d926019190773e8a067462625314d3548c987 \
> --control-plane 
[preflight] Running pre-flight checks
	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR CRI]: container runtime is not running: output: time="2023-12-28T10:25:29+08:00" level=fatal msg="validate service connection: CRI v1 runtime API is not implemented for endpoint \"unix:///var/run/containerd/containerd.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService", error: exit status 1
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher
```
[root@node2 ~]# systemctl enable kubelet.service
修改containerd配置文件，重启containerd
修改socket文件路径
```
cat /etc/containerd/config.toml 
[grpc]
  address = "/var/run/containerd/containerd.sock"
#  address = "/run/containerd/containerd.sock"
```

##### 九、kubeadm join master时报错(failure loading certificate for CA)手动证书发放
```
[root@node2 ~]#   kubeadm join cluster-endpoint:6443 --token b8ufi2.1qs3rw48dehh5yqx \
> --discovery-token-ca-cert-hash sha256:05fff7ddf0b5a6b9d4a7fd40de6d926019190773e8a067462625314d3548c987 \
> --control-plane 
[preflight] Running pre-flight checks
	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
error execution phase preflight: 
One or more conditions for hosting a new control plane instance is not satisfied.

[failure loading certificate for CA: couldn't load the certificate file /etc/kubernetes/pki/ca.crt: open /etc/kubernetes/pki/ca.crt: no such file or directory, failure loading key for service account: couldn't load the private key file /etc/kubernetes/pki/sa.key: open /etc/kubernetes/pki/sa.key: no such file or directory, failure loading certificate for front-proxy CA: couldn't load the certificate file /etc/kubernetes/pki/front-proxy-ca.crt: open /etc/kubernetes/pki/front-proxy-ca.crt: no such file or directory, failure loading certificate for etcd CA: couldn't load the certificate file /etc/kubernetes/pki/etcd/ca.crt: open /etc/kubernetes/pki/etcd/ca.crt: no such file or directory]
Please ensure that:
* The cluster has a stable controlPlaneEndpoint address.
* The certificates that must be shared among control plane instances are provided.

To see the stack trace of this error execute with --v=5 or higher
```
相关证书文件没有拷贝到要加入的master节点上，创建目录从master节点拷贝证书到新的master节点上，再重新join
[root@node2 ~]# mkdir -p  /etc/kubernetes/pki/etcd
[root@master pki]# scp -rp /etc/kubernetes/pki/ca.* node2:/etc/kubernetes/pki
[root@master pki]#  scp -rp /etc/kubernetes/pki/sa.* node2:/etc/kubernetes/pki
[root@master pki]#  scp -rp /etc/kubernetes/pki/front-proxy-ca.* node2:/etc/kubernetes/pki
[root@master pki]#  scp -rp /etc/kubernetes/pki/etcd/ca.* node2:/etc/kubernetes/pki/etcd

##### 十、kubectl get node报错(server 192.168.X.X:6443 was refused -did you specify)
查看端口没有被监听
netstat -tunlp | grep 6443
查看kubelet服务是running状态
systemctl status kubelet
journalctl -xeu kubelet输出如下
Aug 30 14:26:36 k8smaster kubelet[6876]: E0830 14:26:36.008302    6876 kubelet.go:2248] node "k8smaster" not found
提示找不到k8smaster节点
网上查找下面几种情况也可能导致这个报错
1. 查看hosts文件和ip地址，都正常
2. 查看.kube目录配置文件，正常
3. image丢失，没有丢失
4. 根分区占满，没有占满
5. 查看cat /etc/kubernetes/kubelet.conf 配置文件的node名和server的ip地址
查看各组件状态，apiserver启动正常，过一会apiserver失败，etcd启动失败
查找apiserver的容器id
```
[root@k8smaster ~]# docker ps -a
CONTAINER ID        IMAGE                                               COMMAND                  CREATED             STATUS                       PORTS               NA
MES551661d93303        2c4adeb21b4f                                        "etcd --advertise-cl…"   2 minutes ago       Exited (2) 2 minutes ago                         k
8s_etcd_etcd-k8smaster_kube-system_aef1d0dcc0b4258f3863a07b0f691b97_110fe4fa6faeab0        201c7a840312                                        "kube-apiserver --ad…"   4 minutes ago       Exited (255) 4 minutes ago                       k
8s_kube-apiserver_kube-apiserver-k8smaster_kube-system_589b0a320bff7edb0b69c0c65ce3f075_111
```
查看apiserver的logs
```
docker logs fe4fa6faeab0
W0830 06:25:56.434071       1 clientconn.go:1251] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 0  <nil>}. Err :connection error: desc = "transpor
t: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...W0830 06:25:56.443032       1 clientconn.go:1251] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 0  <nil>}. Err :connection error: desc = "transpor
```
查看etcd的logs
```
[root@k8smaster ~]# docker logs 551661d93303
panic: freepages: failed to get all reachable pages (page 3482471299534041133: out of bounds: 594)

goroutine 64 [running]:
github.com/coreos/etcd/cmd/vendor/github.com/coreos/bbolt.(*DB).freepages.func2(0xc4200805a0)
	/tmp/etcd-release-3.3.10/etcd/release/etcd/gopath/src/github.com/coreos/etcd/cmd/vendor/github.com/coreos/bbolt/db.go:976 +0xfb
created by github.com/coreos/etcd/cmd/vendor/github.com/coreos/bbolt.(*DB).freepages
	/tmp/etcd-release-3.3.10/etcd/release/etcd/gopath/src/github.com/coreos/etcd/cmd/vendor/github.com/coreos/bbolt/db.go:974 +0x1b7
```
此panic是因为虚拟机异常掉电，导致etcd数据损坏导致的
在故障节点上停止etcd服务并删除损坏的etcd数据，这时etcd本身没有启动，将数据备份，然后reload，重启kubelet
>注意:这样会导致集群配置文件都丢失
[root@k8smaster ~]# mv /var/lib/etcd/member /var/lib/etcd/member.bak
[root@k8smaster ~]# systemctl daemon-reload
[root@k8smaster ~]# systemctl restart kubelet

##### 十一、健康检查发现controller-manager、scheduler状态Unhealthy，查看master组件状态时出现错误
[root@master ~]# kubectl get cs
NAME                 STATUS      MESSAGE                                                                                     ERROR
scheduler            Unhealthy   Get http://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: connect: connection refused   
controller-manager   Unhealthy   Get http://127.0.0.1:10252/healthz: dial tcp 127.0.0.1:10252: connect: connection refused   
etcd-0               Healthy     {"health":"true"}  
修改/etc/kubernetes/manifests/kube-controller-manager.yaml、/etc/kubernetes/manifests/kube-scheduler.yaml文件
[root@master manifests]# cat kube-controller-manager.yaml | grep port
    - --port=10252
        port: 10257
[root@master manifests]# cat kube-scheduler.yaml | grep port
    - --port=10251
        port: 10259
修改完成后，重新apply即可，再次检查健康状态
[root@master manifests]# kubectl apply -f kube-scheduler.yaml 
pod/kube-scheduler created
[root@master manifests]# kubectl apply -f kube-controller-manager.yaml 
pod/kube-controller-manager created
[root@master manifests]# kubectl get cs
NAME                 STATUS    MESSAGE             ERROR
scheduler            Healthy   ok                  
controller-manager   Healthy   ok                  
etcd-0               Healthy   {"health":"true"}   

##### 十二、查看kube-controller-manager状态为CrashLoopBackOff
[root@master ~]# kubectl get pods -n kube-system
kube-controller-manager                 0/1     CrashLoopBackOff    12         38m
使用如下命令查看日志报错，找不到/etc/kubernetes/pki/front-proxy-ca.crt文件
[root@master ~]# kubectl logs -n kube-system kube-controller-manager
I0822 10:34:23.104970       1 serving.go:312] Generated self-signed cert in-memory
unable to create request header authentication config: open /etc/kubernetes/pki/front-proxy-ca.crt: no such file or directory
使用如下命令查看kube-controller-manager运行在了node节点
[root@master ~]# kubectl get pod -n kube-system -o wide
kube-controller-manager                 0/1     CrashLoopBackOff    13         43m     192.168.189.202   node2    <none>           <none>
此时，master运行到了其他的node节点上，故目录下没有相应的文件，删掉，指定运行在master主机上即可
[root@master manifests]# kubectl delete endpoints -n kube-system kube-controller-manager
endpoints "kube-controller-manager" deleted
指定address地址
删除这个pod
[root@master manifests]# kubectl delete -f kube-controller-manager.yaml 
pod "kube-controller-manager" deleted
再启动这个pod
[root@master manifests]# kubectl apply -f kube-controller-manager.yaml 
pod/kube-controller-manager created
检查仍然找不到/etc/kubernetes/pki/front-proxy-ca.crt文件
[root@master manifests]# kubectl logs kube-controller-manager -n kube-system
I0822 11:57:57.489290       1 serving.go:312] Generated self-signed cert in-memory
unable to create request header authentication config: open /etc/kubernetes/pki/front-proxy-ca.crt: no such file or directory
查看kube-apiserver-master 还是运行在node2节点上？？？
[root@master manifests]# kubectl get pod -n kube-system -o wide
kube-controller-manager                 0/1     CrashLoopBackOff    3          83s     192.168.189.202   node2    <none>           <none>
分析kube-controller-manager启动参数，leader-elect设置为true，此为高可用场景下多个kube-controller-manager实例竞争选举哪个实例为leader角色的开关，开启
时kube-controller-manger实例启动时会连接kube-api竞争创建名为kube-controller-manager的endpoint，创建成功的kube-controller-manger实例为leader，其他
实例为backup，同时leader实例需要定期更新此endpoint，维持leader地位。
此环境为非高可用环境，修改leader-elect为false避免kube-controller-manager定期去连接kube-api更新endpoint，理论也可以避免renew超时退出问题
[root@master manifests]# cat kube-scheduler.yaml | grep leader
    - --leader-elect=true
[root@master manifests]# cat kube-controller-manager.yaml | grep leader
    - --leader-elect=false
##### 十三、设置cgroup为systemd导致docker重启失败
因为/lib/systemd/system/docker.service 和daemon.json有冲突导致
vim /lib/systemd/system/docker.service文件将如下行删除即可
--exec-opt native.cgroupdriver=cgroupfs \
systemctl daemon-reload
systemctl restart docker

##### 十四、kubeadm安装k8s找不到bridge
[root@node1 ~]# sysctl net.bridge.bridge-nf-call-iptables=1
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directory
需要先加载模块
[root@node1 net]# modprobe br_netfilter
再执行
[root@node1 net]# sysctl net.bridge.bridge-nf-call-ip6tables=1
net.bridge.bridge-nf-call-ip6tables = 1
[root@node1 net]# sysctl net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-iptables = 1

##### 十五、kubeadm_token过期

**默认情况下，令牌会在 24 小时后过期。如果要在当前令牌过期后将节点加入集群， 则可以通过在控制平面节点上运行以下命令来创建新令牌**

###### 1、分别获取token和ca-cert-hash的值
1. 如果没有令牌，可以通过在控制平面节点上运行以下命令来获取令牌
`kubeadm token create`
```
[root@master ~]# kubeadm token create
md76nd.39rycar2x8nj697x
```
2. 如果没有 --discovery-token-ca-cert-hash 的值，则可以通过在控制平面节点上执行以下命令链来获取它
`openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'`
```
[root@master ~]# openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'
05fff7ddf0b5a6b9d4a7fd40de6d926019190773e8a067462625314d3548c987
```
3. 查看token
`kubeadm token list`
```
[root@master ~]# kubeadm token list
TOKEN                     TTL  EXPIRES                USAGES                   DESCRIPTION  EXTRA GROUPS
md76nd.39rycar2x8nj697x   23h  2024-01-12T02:05:38Z   authentication,signing   <none>       system:bootstrappers:kubeadm:default-node-token
```


###### 2、直接打印join的命令
一般不用上面的分别获取 token 和 ca-cert-hash 方式，可以执行以下命令一气呵成：
`kubeadm token create --print-join-command`
```
[root@master ~]# kubeadm token create --print-join-command
kubeadm join cluster-endpoint:6443 --token 2k9wqo.o8hsb9lipavh7dm5 --discovery-token-ca-cert-hash sha256:05fff7ddf0b5a6b9d4a7fd40de6d926019190773e8a067462625314d3548c987 
```
如果是需要加入master节点在kubeadm join命令最后跟上--control-plane 
加入control-plane节点前，需要先拷贝control-plane相关ca证书再join集群

>由于集群节点通常是按顺序初始化的，CoreDNS Pod 很可能都运行在第一个控制面节点上。 为了提供更高的可用性，请在加入至少一个新节点后 使用 kubectl -n kube-system rollout restart deployment coredns 命令，重新平衡这些 CoreDNS Pod。 

##### 十六、k8s更新ca证书过期时间
随着 kubernetes 集群的使用，某一天证书过期了，此时 kubernetes 集群将无法正常使用，比如：kubectl 命令执行会产生错误（You must be logged in to the server(unauthorized)）、通过 k8s 接口访问资源时出现“证书过期”的错误等。

###### 1、查看证书是否过期
openssl x509 -in *.crt -noout -text | grep -i not
[root@master pki]# openssl x509 -in /etc/kubernetes/pki/ca.crt -noout -text | grep -i not
            Not Before: Dec 28 09:40:14 2023 GMT
            Not After : Dec 25 09:40:14 2033 GMT
[root@master pki]# openssl x509 -in apiserver.crt -noout -text | grep -i not
            Not Before: Dec 28 09:40:14 2023 GMT
            Not After : Dec 27 09:40:14 2024 GMT

如果是使用kubeadm部署的集群，可以使用`kubeadm  certs check-expiration`命令查看证书到期时间。1.19版本之前使用`kubeadm alpha certs check-expiration`
```
[root@master ~]# kubeadm  certs check-expiration
[check-expiration] Reading configuration from the cluster...
[check-expiration] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'

CERTIFICATE                EXPIRES                  RESIDUAL TIME   CERTIFICATE AUTHORITY   EXTERNALLY MANAGED
admin.conf                 Apr 07, 2025 06:41 UTC   364d            ca                      no      
apiserver                  Apr 07, 2025 06:41 UTC   364d            ca                      no      
apiserver-etcd-client      Apr 07, 2025 06:41 UTC   364d            etcd-ca                 no      
apiserver-kubelet-client   Apr 07, 2025 06:41 UTC   364d            ca                      no      
controller-manager.conf    Apr 07, 2025 06:41 UTC   364d            ca                      no      
etcd-healthcheck-client    Apr 07, 2025 06:41 UTC   364d            etcd-ca                 no      
etcd-peer                  Apr 07, 2025 06:41 UTC   364d            etcd-ca                 no      
etcd-server                Apr 07, 2025 06:41 UTC   364d            etcd-ca                 no      
front-proxy-client         Apr 07, 2025 06:41 UTC   364d            front-proxy-ca          no      
scheduler.conf             Apr 07, 2025 06:41 UTC   364d            ca                      no      

CERTIFICATE AUTHORITY   EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGED
ca                      Dec 30, 2033 16:06 UTC   9y              no      
etcd-ca                 Dec 30, 2033 16:06 UTC   9y              no      
front-proxy-ca          Dec 30, 2033 16:06 UTC   9y              no 
```
上面的列表中没有包含 kubelet.conf，因为 kubeadm 将 kubelet 配置为自动更新证书。 轮换的证书位于目录 /var/lib/kubelet/pki。

###### 2、更新证书前备份集群的证书和配置
kubeadm config view > kubeadm-cluster.yaml 
cp -rp /etc/kubernetes/ /etc/kubernetes.bak
cp -rp /var/lib/etcd /var/lib/etcd.bak

###### 3、更新证书
使用`kubeadm  certs renew all`更新证书，这个命令将更新所有证书，并自动更新kubeconfig文件和各种配置文件。1.19版本之前使用`kubeadm alpha certs renew all`
如果你运行了一个 HA 集群，这个命令需要在所有控制面板节点上执行。
```
[root@master ~]# kubeadm  certs renew all
[renew] Reading configuration from the cluster...
[renew] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'

certificate embedded in the kubeconfig file for the admin to use and for kubeadm itself renewed
certificate for serving the Kubernetes API renewed
certificate the apiserver uses to access etcd renewed
certificate for the API server to connect to kubelet renewed
certificate embedded in the kubeconfig file for the controller manager to use renewed
certificate for liveness probes to healthcheck etcd renewed
certificate for etcd nodes to communicate with each other renewed
certificate for serving etcd renewed
certificate for the front proxy client renewed
certificate embedded in the kubeconfig file for the scheduler manager to use renewed

Done renewing certificates. You must restart the kube-apiserver, kube-controller-manager, kube-scheduler and etcd, so that they can use the 
new certificates.
```

###### 4、重启所有master的所有组件
更新证书后需要重启控制面 Pod。因为动态证书重载目前还不被所有组件和证书支持，所有这项操作是必须的。 静态 Pod 是被本地 kubelet 而不是 API 服务器管理，所以 kubectl 不能用来删除或重启他们。 要重启静态 Pod 你可以临时将清单文件从 /etc/kubernetes/manifests/ 移除并等待 20 秒 （参考 KubeletConfiguration 结构中的 fileCheckFrequency 值）。如果 Pod 不在清单目录里，kubelet 将会终止它。 在另一个 fileCheckFrequency 周期之后你可以将文件移回去，kubelet 可以完成 Pod 的重建，而组件的证书更新操作也得以完成。
docker ps | grep -E 'k8s_kube-apiserver|k8s_kube-controller-manager|k8s_kube-scheduler|k8s_etcd_etcd' | awk '{print $1}' | xargs docker restart 

###### 5、更新~/.kube/config文件
sudo mv $HOME/.kube/config $HOME/.kube/config.bak
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config


>注意事项:
更新证书只需在master上更新，node节点上的kubelet证书默认自动轮换更新，无需关心过期问题
执行k8s版本升级也会自动更新证书
更新证书可能会导致短暂的服务中断
更新证书前，做好关机数据备份

###### 6、更新kubelet.conf
默认情况下，kubeadm 使用 /etc/kubernetes/kubelet.conf 中指定的 /var/lib/kubelet/pki/kubelet-client-current.pem 符号链接来配置 kubelet 自动轮换客户端证书。如果此轮换过程失败，你可能会在 kube-apiserver 日志中看到诸如 x509: certificate has expired or is not yet valid 之类的错误。要解决此问题，你必须执行以下步骤：

    从故障节点备份和删除 /etc/kubernetes/kubelet.conf 和 /var/lib/kubelet/pki/kubelet-client*。
    在集群中具有 /etc/kubernetes/pki/ca.key 的、正常工作的控制平面节点上 执行 kubeadm kubeconfig user --org system:nodes --client-name system:node:$NODE > kubelet.conf。 $NODE 必须设置为集群中现有故障节点的名称。 手动修改生成的 kubelet.conf 以调整集群名称和服务器端点， 或传递 kubeconfig user --config （请参阅为其他用户生成 kubeconfig 文件）。 如果你的集群没有 ca.key，你必须在外部对 kubelet.conf 中的嵌入式证书进行签名。

    将得到的 kubelet.conf 文件复制到故障节点上，作为 /etc/kubernetes/kubelet.conf。
    在故障节点上重启 kubelet（systemctl restart kubelet），等待 /var/lib/kubelet/pki/kubelet-client-current.pem 重新创建。

    手动编辑 kubelet.conf 指向轮换的 kubelet 客户端证书，方法是将 client-certificate-data 和 client-key-data 替换为：

    client-certificate: /var/lib/kubelet/pki/kubelet-client-current.pem
    client-key: /var/lib/kubelet/pki/kubelet-client-current.pem

    重新启动 kubelet。
    确保节点状况变为 Ready

即admin.conf中的client-certificate-data 与 client-key-data  替换kubelet.conf文件的client-certificate 与 client-key
重启 kubelet（systemctl restart kubelet），等待 /var/lib/kubelet/pki/kubelet-client-current.pem 重新创建。再将配置文件改回去
    client-certificate: /var/lib/kubelet/pki/kubelet-client-current.pem
    client-key: /var/lib/kubelet/pki/kubelet-client-current.pem
再重启 kubelet

##### 十七、crictl命令报错
执行crictl报错如下
```shell
[root@master kubernetes]# crictl ps
WARN[0000] runtime connect using default endpoints: [unix:///var/run/dockershim.sock unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead. 
E0102 14:41:46.371778    3392 remote_runtime.go:390] "ListContainers with filter from runtime service failed" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing
 dial unix /var/run/dockershim.sock: connect: no such file or directory\"" filter="&ContainerFilter{Id:,State:&ContainerStateValue{State:CONTAINER_RUNNING,},PodSandboxId:,LabelSelector:map[string]string{},}"FATA[0000] listing containers: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /var/run/dockershim.sock: connect: no such file or directory" 
```
添加以下文件，默认没有这个文件，然后再执行就不会报错了
```shell
[root@master kubernetes]# cat >/etc/crictl.yaml <<EOF
runtime-endpoint: unix:///run/containerd/containerd.sock
image-endpoint: unix:///run/containerd/containerd.sock
timeout: 0
debug: false
pull-image-on-create: false
EOF
```
套接字文件默认路径为/run/containerd/containerd.sock,我修改为/var/run/containerd/containerd.sock了，/etc/crictl.yaml文件需要根据实际情况进行修改
```shell
[root@master ~]# cat /etc/containerd/config.toml
[grpc]
  address = "/var/run/containerd/containerd.sock"
#  address = "/run/containerd/containerd.sock"
```
使用cricrl config也可以设置各options
```shell
[root@master ~]# crictl config
NAME:
   crictl config - Get and set crictl client configuration options

USAGE:
   crictl config command [command options] [<crictl options>]

EXAMPLE:
   crictl config --set debug=true

CRICTL OPTIONS:
   runtime-endpoint:      Container runtime endpoint
   image-endpoint:        Image endpoint
   timeout:               Timeout of connecting to server (default: 2s)
   debug:                 Enable debug output (default: false)
   pull-image-on-create:  Enable pulling image on create requests (default: false)
   disable-pull-on-run:   Disable pulling image on run requests (default: false)
```
```shell
[root@master kubernetes]# crictl ps
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
ce861c71cfa24       18688a72645c5       5 hours ago         Running             kube-scheduler            6                   760fb0d07e979       kube-scheduler-master
80b37535ee841       aebe758cef4cd       5 hours ago         Running             etcd                      4                   594be2ef4d116       etcd-master
5997a0bc1ef01       b4ea7e648530d       5 hours ago         Running             kube-controller-manager   6                   01654381901fd       kube-controller-manager-master
159ea92fdbe81       e9f4b425f9192       5 hours ago         Running             kube-apiserver            4                   56ce03f6c0a10       kube-apiserver-master
```
